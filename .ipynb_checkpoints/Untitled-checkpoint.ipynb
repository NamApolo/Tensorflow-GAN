{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 1e-3\n",
    "epoch = 10000\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "z_in = tf.placeholder(tf.float32, shape=[batch_size, 100])\n",
    "\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "''' \n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x)\n",
    "'''\n",
    "\n",
    "def generator(z):\n",
    "    \n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=z, num_outputs=7*7*128, activation_fn=tf.nn.relu, \\\n",
    "                                                normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                                weights_initializer=initializer,scope=\"g_fc1\")\n",
    "        fc1 = tf.reshape(fc1, shape=[batch_size, 7, 7, 128])\n",
    "        conv1 = tf.contrib.layers.conv2d(fc1, num_outputs=4*64, kernel_size=5, stride=1, padding=\"SAME\",    \\\n",
    "                                        activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv1\")\n",
    "        conv1 = tf.reshape(conv1, shape=[batch_size,14,14,64])\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1, num_outputs=4*32, kernel_size=5, stride=1, padding=\"SAME\", \\\n",
    "                                        activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv2\")\n",
    "\n",
    "        conv2 = tf.reshape(conv2, shape=[batch_size,28,28,32])\n",
    "        conv3 = tf.contrib.layers.conv2d(conv2, num_outputs=1, kernel_size=5, stride=1, padding=\"SAME\", \\\n",
    "                                        activation_fn=tf.nn.tanh,scope=\"g_conv3\")\n",
    "\n",
    "        return conv3\n",
    "\n",
    "\n",
    "def discriminator(tensor,reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=tensor, num_outputs=32, kernel_size=5, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=lrelu,weights_initializer=initializer,scope=\"d_conv1\")\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=5, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=lrelu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv2\")\n",
    "        fc1 = tf.reshape(conv2, shape=[batch_size, 7*7*64])\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512,reuse=reuse, activation_fn=lrelu, \\\n",
    "                                                normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                                weights_initializer=initializer,scope=\"d_fc1\")\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=reuse, activation_fn=tf.nn.sigmoid,\\\n",
    "                                                weights_initializer=initializer,scope=\"d_fc2\")\n",
    "\n",
    "        return fc2\n",
    "\n",
    "\n",
    "g_out = generator(z_in)\n",
    "d_out_fake = discriminator(g_out)\n",
    "d_out_real = discriminator(x_image,reuse=True)\n",
    "\n",
    "# loss & optimizer\n",
    "\n",
    "disc_loss = tf.reduce_sum(tf.square(d_out_real-1) + tf.square(d_out_fake))/2\n",
    "gen_loss = tf.reduce_sum(tf.square(d_out_fake-1))/2\n",
    "\n",
    "tvars = tf.trainable_variables() \n",
    "\n",
    "gen_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator\") \n",
    "dis_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator\") \n",
    "\n",
    "d_optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "g_optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "d_grads = d_optimizer.compute_gradients(disc_loss,dis_variables) #Only update the weights for the discriminator network.\n",
    "g_grads = g_optimizer.compute_gradients(gen_loss,gen_variables) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = d_optimizer.apply_gradients(d_grads)\n",
    "update_G = g_optimizer.apply_gradients(g_grads)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        z_input = np.random.uniform(0,1.0,size=[batch_size,100]).astype(np.float32)\n",
    "\n",
    "        _, d_loss = sess.run([update_D,disc_loss],feed_dict={x: batch[0], z_in: z_input})\n",
    "        \n",
    "        for j in range(4):\n",
    "            _, g_loss = sess.run([update_G,gen_loss],feed_dict={z_in: z_input})\n",
    "\n",
    "        print(\"i: {} / d_loss: {} / g_loss: {}\".format(i,np.sum(d_loss)/batch_size, np.sum(g_loss)/batch_size))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "\n",
    "            gen_o = sess.run(g_out,feed_dict={z_in: z_input})\n",
    "            #result = plt.imshow(gen_o[0][:, :, 0], cmap=\"gray\")\n",
    "            plt.imsave(\"{}.png\".format(i),gen_o[0][:, :, 0], cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
