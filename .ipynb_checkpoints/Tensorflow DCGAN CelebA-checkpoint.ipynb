{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* site : http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Load CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SavePath = 'CelebA_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SavePath):\n",
    "    # Dropbox Link can be terminated\n",
    "    link = 'https://www.dropbox.com/s/gxjc9p7s6xmo09k/CelebA_Align.zip?dl=1'\n",
    "    url = urllib.request.urlopen(link)\n",
    "\n",
    "    filesize = int(url.headers['Content-Length'])\n",
    "    filename = 'CelebA_Align.7z'\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        downloaded = 0\n",
    "        block_size = 8192\n",
    "        status_width = 70\n",
    "        while True:\n",
    "            buffer = url.read(block_size)\n",
    "            if not buffer:\n",
    "                break\n",
    "\n",
    "            downloaded += len(buffer)\n",
    "            f.write(buffer)\n",
    "            status = ((\"[%-\" + str(status_width + 1) + \"s] %3.2f%%\") %\n",
    "              ('=' * int(downloaded / filesize * status_width) + '>', downloaded * 100 / filesize))\n",
    "            display.clear_output(wait=True)\n",
    "            print(status, end='')\n",
    "\n",
    "    with zipfile.ZipFile(filename) as zf:\n",
    "        print('\\nExtracting %s ...' % filename)\n",
    "        zf.extractall(SavePath)\n",
    "        print('Done.')\n",
    "\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_names = []\n",
    "file_tags = []\n",
    "with open(SavePath + 'list_attr_celeba.txt') as f:\n",
    "    data_size = int(f.readline())\n",
    "    tag_list = np.array(f.readline().split())\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        file_name, *tagging = line.split()\n",
    "        file_names.append(file_name)\n",
    "        file_tags.append((np.array(tagging).astype(int) == 1).astype(int))\n",
    "        \n",
    "    file_names = np.array(file_names)\n",
    "    file_tags = np.array(file_tags)\n",
    "    \n",
    "file_size = len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_load(name, width, height):\n",
    "    img = Image.open(SavePath + 'img_align_celeba/' + name)\n",
    "    img = img.resize((width, height), resample=Image.ANTIALIAS)\n",
    "    pixel = np.array([x for x in img.getdata()])\n",
    "    return pixel.reshape(width, height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_set = {}\n",
    "def batch_from_dataset(names, width, height):\n",
    "    batch = []\n",
    "    for name in names:\n",
    "        if name not in loaded_set:\n",
    "            loaded_set[name] = image_load(name, width, height)\n",
    "        batch.append(loaded_set[name])\n",
    "        \n",
    "    return np.array(batch) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Function Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/GunhoChoi/LSGAN_TF/blob/master/LSGAN/LSGAN_TF.ipynb\n",
    "def LeakyReLU(x, leak=0.2, name='LeakyReLU'):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(x, output_size, initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    w = tf.get_variable(name + '_weight', [x.get_shape()[1], output_size], initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', [output_size], initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    l = tf.nn.bias_add(tf.matmul(x, w), b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization, name=name + '_batch_norm')\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_2d(x, kernel_size, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = int(x.get_shape()[-1])\n",
    "\n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-1], initializer=initializer)\n",
    "    c = tf.nn.conv2d(x, w, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization, name=name + '_batch_norm')\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv_2d(x, kernel_size, output_shape, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = output_shape[-1]\n",
    "    if kernel_size[3] == -1: kernel_size[3] = int(x.get_shape()[-1])\n",
    "    \n",
    "    if type(output_shape) == tuple: output_shape = list(output_shape)\n",
    "    if output_shape[0] == -1: output_shape[0] = tf.shape(x)[0]\n",
    "    \n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-2], initializer=initializer)\n",
    "    c = tf.nn.conv2d_transpose(x, w, output_shape=output_shape, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization, name=name + '_batch_norm')\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Maker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMaker(object):\n",
    "    def __init__(self, layers_shape):\n",
    "        self.layers_shape = layers_shape\n",
    "        \n",
    "    def __call__(self, x, name, dropout_list=None, reuse=False):\n",
    "        parameters = set()\n",
    "        layers = set()\n",
    "        \n",
    "        last_layer = x\n",
    "        \n",
    "        dropout = None\n",
    "                \n",
    "        # scope set\n",
    "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "            # create layers\n",
    "            for i, (layer_type, *layer_shape) in enumerate(self.layers_shape):\n",
    "                \n",
    "                '''\n",
    "                create matching layer\n",
    "                \n",
    "                c2l  : Convolutional 2 Dimention Layer\n",
    "                dc2l : Deconvolutional 2 Dimention Layer\n",
    "                fcl  : Fully Connected Layer(Dense) Layer\n",
    "                mpl  : Max Pooling Layer\n",
    "                rs   : Reshape\n",
    "                flat : Flatten\n",
    "                '''\n",
    "                if layer_type == 'c2l': # Convolutional 2D Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = conv_2d(x=last_layer, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'dc2l': # Deconvolutional 2D Layer\n",
    "                    kernel_shape, output_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = deconv_2d(x=last_layer, output_shape=output_shape, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'fcl': # Fully Connected Layer\n",
    "                    output_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = fully_connected_layer(x=last_layer, \\\n",
    "                                                output_size=output_shape, name=str(i) + '_fc', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'mpl': # Max Pooling Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer = tf.nn.max_pool(input=x, ksize=kernel_shape, strides=stride_shape, \\\n",
    "                                    name=str(i) + '_mp_layer', **parmas)\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'rs': # Reshape Layer\n",
    "                    reshape = layer_shape[0]\n",
    "                    last_layer = tf.reshape(last_layer, reshape, name=str(i) + '_reshape')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'flat': # Flat\n",
    "                    try:\n",
    "                        flat_size = int(np.prod(last_layer.get_shape()[1:]))\n",
    "                    except:\n",
    "                        flat_size = tf.reduce_prod(tf.shape(last_layer)[1:])\n",
    "                        \n",
    "                    last_layer = tf.reshape(last_layer, (-1, flat_size), name=str(i) + '_flat')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                # Dropout Layer\n",
    "                if type(dropout) == int: # var is index\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout_list[dropout], name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                elif type(dropout) == float: # var is constant value\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout, name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                \n",
    "                # initialize vars\n",
    "                layer_shape = \\\n",
    "                kernel_shape = \\\n",
    "                stride_shape = \\\n",
    "                dropout = \\\n",
    "                params = None\n",
    "                    \n",
    "            return last_layer, layers, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Util Function Implment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ArrayToImage(arr):\n",
    "    img = Image.fromarray(np.uint8(arr))\n",
    "    return img\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "\n",
    "# image list display function for 'Jupytor notebook'\n",
    "def DisplayHorizontal(images, header=None, width=\"100%\", figsize=(20, 20), fontsize=20, depth=1):\n",
    "    num_images = len(images)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        \n",
    "        fig.add_subplot(depth, num_images/depth, i+1)\n",
    "        plt.axis('off')\n",
    "        if header != None:\n",
    "            plt.title(header[i], fontsize=fontsize)\n",
    "        plt.imshow(image, cmap='Greys_r', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_size = len(tag_list)\n",
    "image_width = 64\n",
    "image_height = 64\n",
    "image_depth = 3\n",
    "image_size = image_width * image_height * image_depth\n",
    "\n",
    "# z: latent random variable\n",
    "z_var = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_depth = 5\n",
    "dis_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 shape: (64, 64)\n",
      "2 shape: (32, 32)\n",
      "3 shape: (16, 16)\n",
      "4 shape: (8, 8)\n",
      "5 shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "for i in range(np.max((gen_depth, dis_depth))):\n",
    "    print('%d shape: (%d, %d)' % \\\n",
    "        (i+1, np.ceil(image_height / 2**i).astype(int), np.ceil(image_width / 2**i).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_layer_shape=(    \n",
    "    ('fcl', 4*4*1024, None, {'batch_normalization': {}, 'activation': tf.nn.relu}),\n",
    "    ('rs', (-1, 4, 4, 1024),),\n",
    "    ('dc2l', (5, 5, -1, -1), (-1, 8, 8, 512), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': tf.nn.relu}),\n",
    "    ('dc2l', (5, 5, -1, -1), (-1, 16, 16, 256), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': tf.nn.relu}),\n",
    "    ('dc2l', (5, 5, -1, -1), (-1, 32, 32, 128), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': tf.nn.relu}),\n",
    "    ('dc2l', (5, 5, -1, -1), (-1, 64, 64, 3), (1, 2, 2, 1), None, {'padding': 'SAME', 'activation': tf.nn.tanh}),\n",
    ")\n",
    "\n",
    "discriminator_layer_shape=(\n",
    "    ('c2l', (5, 5, -1, 64), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (5, 5, -1, 128), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (5, 5, -1, 256), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (5, 5, -1, 512), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('rs', (-1, 4 * 4 * 512),),\n",
    "    #('fcl', 1, None, {'activation': tf.nn.sigmoid})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "beta1=5e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_Maker = ModelMaker(generator_layer_shape)\n",
    "D_Maker = ModelMaker(discriminator_layer_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:%d' % gpu_num):\n",
    "    # Latent Random Variable\n",
    "    Z = tf.placeholder(tf.float32, [None, z_var])\n",
    "\n",
    "    # Generator\n",
    "    X_Fake, G_Layers, G_Params = G_Maker(Z, name='generator')\n",
    "\n",
    "    # For Real Data\n",
    "    X_Real = tf.placeholder(tf.float32, [None, image_height, image_width, image_depth])\n",
    "    \n",
    "    # Discriminator for Fake Data\n",
    "    FC_Fake, D_Fake_Layers, D_Params = D_Maker(X_Fake, name='discriminator')\n",
    "    # Discriminator for Real Data\n",
    "    FC_Real, D_Real_Layers, _ = D_Maker(X_Real, name='discriminator', reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:%d' % gpu_num):\n",
    "    # Fake Outputs\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        Fake, Fake_Logits, *_ = fully_connected_layer(FC_Fake, 1, activation=tf.nn.sigmoid, name='prob')\n",
    "\n",
    "    # Real Outputs\n",
    "    with tf.variable_scope('discriminator', reuse=True):\n",
    "        Real, Real_Logits, *_ = fully_connected_layer(FC_Real, 1, activation=tf.nn.sigmoid, name='prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:%d' % gpu_num):\n",
    "    Optimizer_D = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    Optimizer_G = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "\n",
    "    Loss_D = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Real_Logits, labels=tf.ones_like(Real)) + \\\n",
    "                            tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.zeros_like(Fake)))\n",
    "    Loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.ones_like(Fake)))\n",
    "    \n",
    "    Train_D = Optimizer_D.minimize(Loss_D, var_list=D_Params)\n",
    "    Train_G = Optimizer_G.minimize(Loss_G, var_list=G_Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_size = 100\n",
    "batch_size = 512\n",
    "train_d_count, train_g_count = 1, 2\n",
    "\n",
    "display_count = 10\n",
    "display_with = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_sample = np.random.normal(-1, 1, size=(100, z_var)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.8, allow_growth=True)))\n",
    "saver = tf.train.Saver()\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('result/'):\n",
    "    os.mkdir('result/')\n",
    "if not os.path.exists('result/figures/'):\n",
    "    os.mkdir('result/figures/')\n",
    "if not os.path.exists('result/model/'):\n",
    "    os.mkdir('result/model/')\n",
    "    \n",
    "    \n",
    "sample_figure_path = 'result/figures/'\n",
    "model_path = 'result/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Count: 20\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkEAAAA7CAYAAADM+y2hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP4NSSohWsfAcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbffc71b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [=>                   ] 13312/202599, loss: d=12.6043, g=0.0081"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-73026b214926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Discriminator Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_d_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrain_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoss_D\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX_Real\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_z\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/etri/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/etri/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/etri/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/etri/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/etri/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = np.ceil(len(file_names) / batch_size).astype(int)\n",
    "\n",
    "while epoch < epoch_size:\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    \n",
    "    batch_count = 0    \n",
    "    status = ('start training')\n",
    "    sys.stdout.write(status)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for i in range(count):\n",
    "        s = i * batch_size\n",
    "        e = s + batch_size\n",
    "        \n",
    "        batch_files = file_names[s:e]\n",
    "        batch_train = batch_from_dataset(batch_files, image_width, image_height)\n",
    "        batch_train = batch_train.astype(np.float32) * 2 - 1\n",
    "        batch_count += len(batch_train)\n",
    "        \n",
    "        batch_z = np.random.normal(-1, 1, size=(len(batch_train), z_var)).astype(np.float32)\n",
    "        \n",
    "        # Discriminator Train\n",
    "        for _ in range(train_d_count):\n",
    "            _, loss_d = sess.run([Train_D, Loss_D], feed_dict={X_Real: batch_train, Z: batch_z})\n",
    "            d_loss += [loss_d]\n",
    "        \n",
    "        # Generator Train\n",
    "        for _ in range(train_g_count):\n",
    "            _, loss_g = sess.run([Train_G, Loss_G], feed_dict={Z: batch_z})\n",
    "            g_loss += [loss_g]\n",
    "                        \n",
    "        if (i+1) % display_count == 0:\n",
    "            gen_mnist = sess.run(X_Fake, feed_dict={Z: z_sample})\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            DisplayHorizontal([ArrayToImage((x * 0.5 + 0.5) * 255) for x in gen_mnist], depth=10, size=(16, 16))\n",
    "            print('Epoch: %d, Count: %d' % (epoch+1, i+1))\n",
    "            display.display(plt.gcf())\n",
    "            plt.savefig(sample_figure_path + 'DCGAN_epoch_%d_batch_%d.png' % (epoch+1, i+1))\n",
    "            plt.close()\n",
    "        else:\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.flush()\n",
    "        status = ((\"Epoch: %d [%-\" + str(display_with + 1) + \"s] %d/%d, loss: d=%.4f, g=%.4f\") %\n",
    "          (epoch+1, '=' * int(batch_count / file_size * display_with) + '>', batch_count, file_size, np.mean(loss_d), np.mean(loss_g)))\n",
    "        sys.stdout.write(status)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    saver.save(sess, model_path + 'DCGAN_CelebA.ckpt')\n",
    "    \n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
