{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Function Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/GunhoChoi/LSGAN_TF/blob/master/LSGAN/LSGAN_TF.ipynb\n",
    "def LeakyReLU(x, leak=0.2, name='LeakyReLU'):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(x, output_size, initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    w = tf.get_variable(name + '_weight', [x.get_shape()[1], output_size], initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', [output_size], initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    l = tf.nn.bias_add(tf.matmul(x, w), b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization)\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_2d(x, kernel_size, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = int(x.get_shape()[-1])\n",
    "\n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-1], initializer=initializer)\n",
    "    c = tf.nn.conv2d(x, w, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization, name=name + '_layer_batch_norm')\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv_2d(x, kernel_size, output_shape, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = output_shape[-1]\n",
    "    if kernel_size[3] == -1: kernel_size[3] = int(x.get_shape()[-1])\n",
    "    \n",
    "    if type(output_shape) == tuple: output_shape = list(output_shape)\n",
    "    if output_shape[0] == -1: output_shape[0] = tf.shape(x)[0]\n",
    "    \n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-2], initializer=initializer)\n",
    "    c = tf.nn.conv2d_transpose(x, w, output_shape=output_shape, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization)\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Maker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMaker(object):\n",
    "    def __init__(self, layers_shape):\n",
    "        self.layers_shape = layers_shape\n",
    "        \n",
    "    def __call__(self, x, name, dropout_list=None, reuse=False):\n",
    "        parameters = set()\n",
    "        layers = set()\n",
    "        \n",
    "        last_layer = x\n",
    "        \n",
    "        dropout = None\n",
    "                \n",
    "        # scope set\n",
    "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "            # create layers\n",
    "            for i, (layer_type, *layer_shape) in enumerate(self.layers_shape):\n",
    "                \n",
    "                '''\n",
    "                create matching layer\n",
    "                \n",
    "                c2l  : Convolutional 2 Dimention Layer\n",
    "                dc2l : Deconvolutional 2 Dimention Layer\n",
    "                fcl  : Fully Connected Layer(Dense) Layer\n",
    "                mpl  : Max Pooling Layer\n",
    "                rs   : Reshape\n",
    "                flat : Flatten\n",
    "                '''\n",
    "                if layer_type == 'c2l': # Convolutional 2D Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = conv_2d(x=last_layer, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'dc2l': # Deconvolutional 2D Layer\n",
    "                    kernel_shape, output_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = deconv_2d(x=last_layer, output_shape=output_shape, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'fcl': # Fully Connected Layer\n",
    "                    output_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = fully_connected_layer(x=last_layer, \\\n",
    "                                                output_size=output_shape, name=str(i) + '_fc', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'mpl': # Max Pooling Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer = tf.nn.max_pool(input=x, ksize=kernel_shape, strides=stride_shape, \\\n",
    "                                    name=str(i) + '_mp_layer', **parmas)\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'rs': # Reshape Layer\n",
    "                    reshape = layer_shape[0]\n",
    "                    last_layer = tf.reshape(last_layer, reshape, name=str(i) + '_reshape')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'flat': # Flat\n",
    "                    try:\n",
    "                        flat_size = int(np.prod(last_layer.get_shape()[1:]))\n",
    "                    except:\n",
    "                        flat_size = tf.reduce_prod(tf.shape(last_layer)[1:])\n",
    "                        \n",
    "                    last_layer = tf.reshape(last_layer, (-1, flat_size), name=str(i) + '_flat')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                # Dropout Layer\n",
    "                if type(dropout) == int: # var is index\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout_list[dropout], name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                elif type(dropout) == float: # var is constant value\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout, name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                \n",
    "                # initialize vars\n",
    "                layer_shape = \\\n",
    "                kernel_shape = \\\n",
    "                stride_shape = \\\n",
    "                dropout = \\\n",
    "                params = None\n",
    "                    \n",
    "            return last_layer, layers, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Util Function Implment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1-dimention Array to Image, ex) [784] array -> [28, 28] image\n",
    "from PIL import Image\n",
    "def ArrayToImage(arr):\n",
    "    # 1-dimention array to 2-dimention\n",
    "    size = np.sqrt(arr.shape[0]).astype(int)\n",
    "    arr = arr.reshape(size, size)\n",
    "    \n",
    "    # array to image\n",
    "    img = Image.fromarray(np.uint8(arr))\n",
    "    return img\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "\n",
    "# image list display function for 'Jupytor notebook'\n",
    "def DisplayHorizontal(images, header=None, width=\"100%\", figsize=(20, 20), fontsize=20, depth=1):\n",
    "    num_images = len(images)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        \n",
    "        fig.add_subplot(depth, num_images/depth, i+1)\n",
    "        plt.axis('off')\n",
    "        if header != None:\n",
    "            plt.title(header[i], fontsize=fontsize)\n",
    "        plt.imshow(image, cmap='Greys_r', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def OneHotEncoder(label_size):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(label_size))\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_size = 10 # mnist [0-9]\n",
    "image_size = 784 # mnist [28, 28]\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "image_depth = 1\n",
    "\n",
    "# z: latent random variable\n",
    "z_var = 100\n",
    "z_category = label_size\n",
    "z_weight = 2\n",
    "z = z_var + z_category + z_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generator_layer_shape=(    \n",
    "    ('fcl', 7 * 7 * 64, None, {'batch_normalization': {'momentum': 0.9, 'epsilon': 1e-4}, 'activation': tf.nn.relu}),\n",
    "    ('rs', (-1, 7, 7, 64),),\n",
    "    ('dc2l', (3, 3, -1, -1), (-1, 14, 14, 32), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': tf.nn.relu}),\n",
    "    ('dc2l', (3, 3, -1, -1), (-1, 28, 28, 1), (1, 2, 2, 1), None, {'padding': 'SAME', 'activation': tf.nn.tanh}),\n",
    ")\n",
    "\n",
    "discriminator_layer_shape=(\n",
    "    ('c2l', (3, 3, -1, 32), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (3, 3, -1, 64), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (3, 3, -1, 128), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('rs', (-1, 4 * 4 * 128),),\n",
    "    #('fcl', 1, None, {'activation': tf.nn.sigmoid})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "beta1=5e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_Maker = ModelMaker(generator_layer_shape)\n",
    "D_Maker = ModelMaker(discriminator_layer_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PlaceHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Latent Random Variable\n",
    "    Z_var = tf.placeholder(tf.float32, [None, z_var])\n",
    "    # Data Label Information\n",
    "    Z_category = tf.placeholder(tf.float32, [None, z_category])\n",
    "    # Data Font Information \n",
    "    Z_weight = tf.placeholder(tf.float32, [None, z_weight])\n",
    "\n",
    "    # Concatenate All Variable & Info\n",
    "    # Generator Input / For Fake Data\n",
    "    Z = tf.concat([Z_var, Z_category, Z_weight], axis=1)\n",
    "\n",
    "    # Generator\n",
    "    X_Fake, G_Layers, G_Params = G_Maker(Z, name='generator')\n",
    "\n",
    "    # For Real Data\n",
    "    X_Real = tf.placeholder(tf.float32, [None, image_size])\n",
    "    X_Real_ = tf.reshape(X_Real, [-1, image_width, image_height, image_depth])\n",
    "\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, z_category])\n",
    "    \n",
    "    # Discriminator for Fake Data\n",
    "    FC_Fake, D_Fake_Layers, D_Params = D_Maker(X_Fake, name='discriminator')\n",
    "    # Discriminator for Real Data\n",
    "    FC_Real, D_Real_Layers, _ = D_Maker(X_Real_, name='discriminator', reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Fake Outputs\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        Fake, Fake_Logits, *_ = fully_connected_layer(FC_Fake, 1, activation=tf.nn.sigmoid, name='prob')\n",
    "        Fake_Category, *_ = fully_connected_layer(FC_Fake, z_category, activation=LeakyReLU, name='label')\n",
    "        Fake_Weight, *_ = fully_connected_layer(FC_Fake, z_weight, activation=tf.nn.tanh, name='weight')\n",
    "\n",
    "    # Real Outputs\n",
    "    with tf.variable_scope('discriminator', reuse=True):\n",
    "        Real, Real_Logits, *_ = fully_connected_layer(FC_Real, 1, activation=tf.nn.sigmoid, name='prob')\n",
    "        Real_Category, *_ = fully_connected_layer(FC_Real, z_category, activation=LeakyReLU, name='label')\n",
    "        Real_Weight, *_ = fully_connected_layer(FC_Real, z_weight, activation=tf.nn.tanh, name='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    Optimizer_D = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    Optimizer_G = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "\n",
    "    Loss_D = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Real_Logits, labels=tf.ones_like(Real)) + \\\n",
    "                            tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.zeros_like(Fake)))\n",
    "    Loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.ones_like(Fake)))\n",
    "\n",
    "    Loss_D_Category = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=Real_Category))\n",
    "    Loss_G_Category = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=Z_category, logits=Fake_Category))\n",
    "\n",
    "    Loss_G_Weight = tf.losses.mean_squared_error(Fake_Weight, Z_weight)\n",
    "\n",
    "    Loss_D_Total = Loss_D + Loss_D_Category\n",
    "    Loss_G_Total = Loss_G + Loss_G_Category + Loss_G_Weight\n",
    "    \n",
    "    Train_D = Optimizer_D.minimize(Loss_D_Total, var_list=D_Params)\n",
    "    Train_G = Optimizer_G.minimize(Loss_G_Total, var_list=G_Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oh_encoder = OneHotEncoder(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_size = 100\n",
    "batch_size = 512\n",
    "train_d_count, train_g_count = 4, 1\n",
    "\n",
    "display_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etri/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:6: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "z_sample = np.concatenate((\\\n",
    "    # random var\n",
    "    np.random.normal(0, 1, size=(100, z_var)).astype(np.float32), \\\n",
    "    # label\n",
    "    list(oh_encoder.transform(range(10))) * 10, \\\n",
    "    # font\n",
    "    [[np.linspace(-1,1,10)[i/10], np.linspace(-1,1,10)[i%10]] for i in range(100)]\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.1, allow_growth=True)))\n",
    "\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(epoch_size):\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    d_loss_category = []\n",
    "    g_loss_category = []\n",
    "    \n",
    "    count = int(np.ceil(mnist.train.num_examples / batch_size))\n",
    "    for j in range(count):\n",
    "        batch_train, batch_target = mnist.train.next_batch(batch_size)\n",
    "        batch_train = batch_train.astype(np.float32) * 2 - 1\n",
    "        \n",
    "        batch_z_var = np.random.normal(0, 1, size=(len(batch_train), z_var)).astype(np.float32)\n",
    "        batch_z_category = oh_encoder.transform(np.random.randint(0, 10, size=len(batch_train)))\n",
    "        batch_z_weight = np.random.random(size=[len(batch_train), 2]) * 2 - 1\n",
    "        \n",
    "        batch_z = np.concatenate((batch_z_var, batch_z_category, batch_z_weight), axis=1)\n",
    "        \n",
    "        # Discriminator Train\n",
    "        for _ in range(train_d_count):\n",
    "            _, loss_d, loss_d_category, asdf = sess.run([Train_D, Loss_D, Loss_D_Category, X_Fake], \\\n",
    "                                 feed_dict={X_Real: batch_train, Y: batch_target, Z: batch_z})\n",
    "            d_loss += [loss_d]\n",
    "            d_loss_category += [loss_d_category]\n",
    "\n",
    "        # Generator Train\n",
    "        for _ in range(train_g_count):\n",
    "            _, loss_g, loss_g_category = sess.run([Train_G, Loss_G, Loss_G_Category], \\\n",
    "                                 feed_dict={Z_var: batch_z_var, Z_category: batch_z_category, Z_weight: batch_z_weight})\n",
    "            g_loss += [loss_g]\n",
    "            g_loss_category += [loss_g_category]\n",
    "            \n",
    "    print('Epoch : %d, loss_d & category : %.4f & %.4f, loss_g & category : %.4f & %.4f' % \\\n",
    "          (i, np.mean(d_loss), np.mean(d_loss_category), np.mean(g_loss), np.mean(g_loss_category)))\n",
    "    \n",
    "    if i % display_epoch == 0:\n",
    "        gen_mnist = sess.run(X_Fake, feed_dict={Z: z_sample})\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        DisplayHorizontal([ArrayToImage((x * 0.5 + 0.5) * 255) for x in np.reshape(gen_mnist, (-1, 784))], depth=10)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
